{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S6-T2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOZ9EmRcoUHuK9wT0805hS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathts1507/TSAI-Assignments-EVA4/blob/master/S6_T2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDBIw7jrF-0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5epfqZPGDg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgd-T-dGGDkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Ie6WBGGDnX",
        "colab_type": "code",
        "outputId": "d6196d53-9dd8-48ce-9109-a35bf1001246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YY6RxY6GlFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dropout_value = 0.04\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Convolution Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(dropout_value),\n",
        "            nn.ReLU()\n",
        "        ) # input_side = 28, output_size = 28, RF = 3\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(dropout_value),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 28, RF = 5\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12, RF = 6\n",
        "\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(dropout_value),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 12, RF = 10\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 10, RF = 14\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(dropout_value),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 8, RF = 18\n",
        "\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 6, RF = 22\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 4, RF = 26\n",
        "\n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.AvgPool2d(kernel_size=(4,4)) \n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            # nn.ReLU() NEVER!\n",
        "        ) # output_size = 1, RF = 26\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock8(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG2iWjUGGDqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch.nn.functional as F\n",
        "# dropout_value = 0.1\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         # Input Block\n",
        "#         self.convblock1 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=1, bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(10),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         ) # output_size = 26, RF=3\n",
        "\n",
        "#         # CONVOLUTION BLOCK 1\n",
        "#         self.convblock2 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=1, bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(20),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         ) # output_size = 24, RF=5\n",
        "\n",
        "#         # TRANSITION BLOCK 1\n",
        "#         self.convblock3 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "#         ) # output_size = 24\n",
        "#         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12, RF=5, step=2\n",
        "\n",
        "#         # CONVOLUTION BLOCK 2\n",
        "#         self.convblock4 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n",
        "#             nn.ReLU(),            \n",
        "#             nn.BatchNorm2d(20),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         ) # output_size = 12, RF=9, step=2\n",
        "#         self.convblock5 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "#             nn.ReLU(),            \n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.Dropout(dropout_value)\n",
        "#         ) # output_size = 6, RF=13, step=2\n",
        "#         # self.convblock6 = nn.Sequential(\n",
        "#         #     nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "#         #     nn.ReLU(),            \n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.Dropout(dropout_value)\n",
        "#         # ) # output_size = 6\n",
        "#         # self.convblock7 = nn.Sequential(\n",
        "#         #     nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "#         #     nn.ReLU(),            \n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.Dropout(dropout_value)\n",
        "#         # ) # output_size = 6\n",
        "        \n",
        "#         # OUTPUT BLOCK\n",
        "#         self.gap = nn.Sequential(\n",
        "#             nn.AvgPool2d(kernel_size=6)\n",
        "#         ) # output_size = 1, RF= 23, step=2\n",
        "\n",
        "#         self.convblock8 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "#             # nn.BatchNorm2d(10),\n",
        "#             # nn.ReLU(),\n",
        "#             # nn.Dropout(dropout_value)\n",
        "#         ) \n",
        "\n",
        "\n",
        "#         self.dropout = nn.Dropout(dropout_value)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.convblock1(x)\n",
        "#         x = self.convblock2(x)\n",
        "#         x = self.convblock3(x)\n",
        "#         x = self.pool1(x)\n",
        "#         x = self.convblock4(x)\n",
        "#         x = self.convblock5(x)\n",
        "#         # x = self.convblock6(x)\n",
        "#         # x = self.convblock7(x)\n",
        "#         x = self.gap(x)        \n",
        "#         x = self.convblock8(x)\n",
        "\n",
        "#         x = x.view(-1, 10)\n",
        "#         return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xWEA4tUGDtf",
        "colab_type": "code",
        "outputId": "8dd908a7-cbde-4cbe-dde7-ff48d757b4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "cpu\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 28, 28]              90\n",
            "       BatchNorm2d-2           [-1, 10, 28, 28]              20\n",
            "           Dropout-3           [-1, 10, 28, 28]               0\n",
            "              ReLU-4           [-1, 10, 28, 28]               0\n",
            "            Conv2d-5           [-1, 10, 28, 28]             900\n",
            "       BatchNorm2d-6           [-1, 10, 28, 28]              20\n",
            "           Dropout-7           [-1, 10, 28, 28]               0\n",
            "              ReLU-8           [-1, 10, 28, 28]               0\n",
            "         MaxPool2d-9           [-1, 10, 14, 14]               0\n",
            "           Conv2d-10           [-1, 10, 12, 12]             900\n",
            "      BatchNorm2d-11           [-1, 10, 12, 12]              20\n",
            "          Dropout-12           [-1, 10, 12, 12]               0\n",
            "             ReLU-13           [-1, 10, 12, 12]               0\n",
            "           Conv2d-14           [-1, 10, 10, 10]             900\n",
            "      BatchNorm2d-15           [-1, 10, 10, 10]              20\n",
            "             ReLU-16           [-1, 10, 10, 10]               0\n",
            "           Conv2d-17             [-1, 10, 8, 8]             900\n",
            "      BatchNorm2d-18             [-1, 10, 8, 8]              20\n",
            "          Dropout-19             [-1, 10, 8, 8]               0\n",
            "             ReLU-20             [-1, 10, 8, 8]               0\n",
            "           Conv2d-21             [-1, 16, 6, 6]           1,440\n",
            "      BatchNorm2d-22             [-1, 16, 6, 6]              32\n",
            "          Dropout-23             [-1, 16, 6, 6]               0\n",
            "             ReLU-24             [-1, 16, 6, 6]               0\n",
            "           Conv2d-25             [-1, 16, 4, 4]           2,304\n",
            "      BatchNorm2d-26             [-1, 16, 4, 4]              32\n",
            "          Dropout-27             [-1, 16, 4, 4]               0\n",
            "             ReLU-28             [-1, 16, 4, 4]               0\n",
            "        AvgPool2d-29             [-1, 16, 1, 1]               0\n",
            "           Conv2d-30             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 7,758\n",
            "Trainable params: 7,758\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.61\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.64\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyYmGhK7GDwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# train_losses = []\n",
        "# test_losses = []\n",
        "# train_acc = []\n",
        "# test_acc = []\n",
        "\n",
        "# def train(model, device, train_loader, optimizer, epoch):\n",
        "#   model.train()\n",
        "#   pbar = tqdm(train_loader)\n",
        "#   correct = 0\n",
        "#   processed = 0\n",
        "#   for batch_idx, (data, target) in enumerate(pbar):\n",
        "#     # get samples\n",
        "#     data, target = data.to(device), target.to(device)\n",
        "\n",
        "#     # Init\n",
        "#     optimizer.zero_grad()\n",
        "#     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "#     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "#     # Predict\n",
        "#     y_pred = model(data)\n",
        "\n",
        "#     # Calculate loss\n",
        "#     loss = F.nll_loss(y_pred, target)\n",
        "#     train_losses.append(loss)\n",
        "\n",
        "#     # Backpropagation\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "#     # Update pbar-tqdm\n",
        "    \n",
        "#     pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "#     correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "#     processed += len(data)\n",
        "\n",
        "#     # print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "#     #     loss, correct, len(train_loader.dataset),\n",
        "#     #     100. * correct / len(train_loader.dataset)))\n",
        "    \n",
        "\n",
        "#     pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "#     train_acc.append(100*correct/processed)\n",
        "\n",
        "# def test(model, device, test_loader):\n",
        "#     model.eval()\n",
        "#     test_loss = 0\n",
        "#     correct = 0\n",
        "#     with torch.no_grad():\n",
        "#         for data, target in test_loader:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             output = model(data)\n",
        "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "#     test_loss /= len(test_loader.dataset)\n",
        "#     test_losses.append(test_loss)\n",
        "\n",
        "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "#         test_loss, correct, len(test_loader.dataset),\n",
        "#         100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "#     test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTR1vPvdNBhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = [[],[],[],[]]\n",
        "train_acc = [[],[],[],[]]\n",
        "test_losses = [[],[],[],[]]\n",
        "test_acc = [[],[],[],[]]\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, L1_weight_decay=0, select_list=0):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "\n",
        "    # L1 loss - if required\n",
        "    if L1_weight_decay:\n",
        "        l1_crit = nn.L1Loss(size_average=False)\n",
        "        reg_loss = 0\n",
        "        for param in model.parameters():\n",
        "            # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.norm\n",
        "            # reg_loss += param.norm(p=1) # can be viable alternative, but have to experiment\n",
        "        # below is not memory efficient as it will create new vector everytime\n",
        "            zero_vector = torch.rand_like(param) * 0\n",
        "            reg_loss += l1_crit(param, zero_vector)        \n",
        "        loss += L1_weight_decay * reg_loss  # add L1 norm regularizer \n",
        "    train_losses[select_list].append(loss)\n",
        "\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc[select_list].append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader, select_list=0):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses[select_list].append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc[select_list].append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NOI17KFNETq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "def plot_history(lists, labels, title=None, xlabel='epochs', ylabel=None, save=False):\n",
        "    '''\n",
        "    Take list of sequences and their respective label and plot them\n",
        "    '''\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if xlabel:\n",
        "        plt.xlabel(xlabel)\n",
        "    if ylabel:\n",
        "        plt.ylabel(ylabel)\n",
        "    for vals, label in zip(lists, labels):\n",
        "        plt.plot(vals, label=label)\n",
        "    plt.legend(loc='best')\n",
        "    if save:\n",
        "        if title:\n",
        "            plt.savefig(title)\n",
        "            print('Fig Saved')\n",
        "        else:\n",
        "            print('Unable to save plot')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "labels = ['Without L1 or L2', 'Only L1 Reg', 'Only L2 Reg', 'Both L1 and L2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx9Kxo9VNGlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mis_classfied(model, test_loader, num=25):  \n",
        "    misclassified_images = torch.rand(25,28,28) * 0\n",
        "    ground_truth = torch.rand(25,1)*0\n",
        "    predicted = torch.rand(25,1)*0\n",
        "    num_false_images = 0\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            false_picker = torch.flatten(pred)-target\n",
        "            index = 0\n",
        "            for val in false_picker:\n",
        "                if (val != 0):\n",
        "                    misclassified_images[num_false_images] = data[index, 0, :, :]\n",
        "                    ground_truth[num_false_images] = target[index]\n",
        "                    predicted[num_false_images] = pred[index]\n",
        "                    num_false_images = num_false_images + 1\n",
        "                    if (num_false_images >= 25): \n",
        "                        break\n",
        "\n",
        "                index = index + 1\n",
        "               \n",
        "            if (num_false_images >= num):\n",
        "                  break\n",
        "    return misclassified_images, ground_truth, predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LbfrzH1NGn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_misclassified(misclassified_images, ground_truth, predicted, title=''):\n",
        "    num_img_rows = 5\n",
        "    num_img_cols = 5\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.set_figheight(10)\n",
        "    fig.set_figwidth(10)\n",
        "    fig.suptitle(title) #super title\n",
        "\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        # plt.tight_layout()\n",
        "        plt.imshow(misclassified_images[i,:,:], cmap='gray', interpolation='none')\n",
        "        plt.title(\"Actual:{},Pred:{}\".format(ground_truth[i].numpy(), predicted[i].numpy()))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIB-BgMXNKP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EPOCHS = 40\n",
        "# L2_weight_decay = 0.0005\n",
        "\n",
        "def run_epochs(EPOCHS=40, L1_weight_decay=0, L2_weight_decay=0, select_list=0):\n",
        "    model =  Net().to(device)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(\"EPOCH:\", epoch)\n",
        "        if epoch < 10:\n",
        "            print('Learning rate :',0.03)\n",
        "            optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9, weight_decay=L2_weight_decay)\n",
        "        elif epoch < 25:\n",
        "            print('Learning rate :',0.003)\n",
        "            optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9, weight_decay=L2_weight_decay)\n",
        "        else:\n",
        "            print('Learning rate :',0.0003)\n",
        "            optimizer = optim.SGD(model.parameters(), lr=0.0003, momentum=0.9, weight_decay=L2_weight_decay)\n",
        "\n",
        "        train(model, device, train_loader, optimizer, epoch, L1_weight_decay=0, select_list=select_list)  # quick fix\n",
        "        test(model, device, test_loader, select_list=select_list) # quick fix\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHL6KBVjNKSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6877cacd-3eae-4b78-a81b-d11c780cdc08"
      },
      "source": [
        "\n",
        "# No L1 or L2\n",
        "model = run_epochs()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss=2.3505678176879883 Batch_id=0 Accuracy=9.38:   0%|          | 1/938 [00:00<01:55,  8.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.02517308108508587 Batch_id=937 Accuracy=93.68: 100%|██████████| 938/938 [01:33<00:00, 10.01it/s]\n",
            "Loss=0.043057795614004135 Batch_id=1 Accuracy=98.44:   0%|          | 2/938 [00:00<01:31, 10.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0464, Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "EPOCH: 1\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.009120987728238106 Batch_id=937 Accuracy=97.98: 100%|██████████| 938/938 [01:34<00:00,  9.92it/s]\n",
            "Loss=0.03542046993970871 Batch_id=0 Accuracy=98.44:   0%|          | 1/938 [00:00<01:41,  9.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0458, Accuracy: 9852/10000 (98.52%)\n",
            "\n",
            "EPOCH: 2\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.008493755012750626 Batch_id=937 Accuracy=98.24: 100%|██████████| 938/938 [01:34<00:00, 10.60it/s]\n",
            "Loss=0.016490323469042778 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:34,  9.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0329, Accuracy: 9898/10000 (98.98%)\n",
            "\n",
            "EPOCH: 3\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.02412530593574047 Batch_id=937 Accuracy=98.44: 100%|██████████| 938/938 [01:34<00:00,  9.94it/s]\n",
            "Loss=0.073514424264431 Batch_id=1 Accuracy=98.44:   0%|          | 1/938 [00:00<01:35,  9.83it/s]   "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0260, Accuracy: 9911/10000 (99.11%)\n",
            "\n",
            "EPOCH: 4\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0026757039595395327 Batch_id=937 Accuracy=98.64: 100%|██████████| 938/938 [01:34<00:00,  9.92it/s]\n",
            "Loss=0.003797139972448349 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:38,  9.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0233, Accuracy: 9929/10000 (99.29%)\n",
            "\n",
            "EPOCH: 5\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.2918512225151062 Batch_id=937 Accuracy=98.70: 100%|██████████| 938/938 [01:35<00:00,  9.82it/s]\n",
            "Loss=0.002229457488283515 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:43,  9.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "EPOCH: 6\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.003140363609418273 Batch_id=937 Accuracy=98.80: 100%|██████████| 938/938 [01:35<00:00,  9.85it/s]\n",
            "Loss=0.015866266563534737 Batch_id=1 Accuracy=100.00:   0%|          | 1/938 [00:00<01:36,  9.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0239, Accuracy: 9922/10000 (99.22%)\n",
            "\n",
            "EPOCH: 7\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.00911177508533001 Batch_id=937 Accuracy=98.94: 100%|██████████| 938/938 [01:35<00:00,  9.85it/s]\n",
            "Loss=0.0014125372981652617 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:37,  9.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0205, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "EPOCH: 8\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0028715182561427355 Batch_id=937 Accuracy=98.86: 100%|██████████| 938/938 [01:34<00:00,  9.93it/s]\n",
            "Loss=0.019437864422798157 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:38,  9.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0234, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "EPOCH: 9\n",
            "Learning rate : 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.02235347405076027 Batch_id=937 Accuracy=98.94: 100%|██████████| 938/938 [01:32<00:00, 10.13it/s]\n",
            "Loss=0.02960946224629879 Batch_id=1 Accuracy=98.44:   0%|          | 2/938 [00:00<01:27, 10.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "EPOCH: 10\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.10256300866603851 Batch_id=937 Accuracy=99.20: 100%|██████████| 938/938 [01:37<00:00, 10.64it/s]\n",
            "Loss=0.009310315363109112 Batch_id=1 Accuracy=98.44:   0%|          | 2/938 [00:00<01:30, 10.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0163, Accuracy: 9945/10000 (99.45%)\n",
            "\n",
            "EPOCH: 11\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.013642320409417152 Batch_id=937 Accuracy=99.18: 100%|██████████| 938/938 [01:36<00:00,  9.75it/s]\n",
            "Loss=0.0032224718015640974 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:49,  8.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 9947/10000 (99.47%)\n",
            "\n",
            "EPOCH: 12\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.17142514884471893 Batch_id=937 Accuracy=99.28: 100%|██████████| 938/938 [01:34<00:00,  9.92it/s]\n",
            "Loss=0.001753029297105968 Batch_id=1 Accuracy=100.00:   0%|          | 1/938 [00:00<01:34,  9.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 9947/10000 (99.47%)\n",
            "\n",
            "EPOCH: 13\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.006573658902198076 Batch_id=937 Accuracy=99.32: 100%|██████████| 938/938 [01:33<00:00, 11.15it/s]\n",
            "Loss=0.03157837316393852 Batch_id=1 Accuracy=100.00:   0%|          | 2/938 [00:00<01:27, 10.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 14\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.31973502039909363 Batch_id=937 Accuracy=99.32: 100%|██████████| 938/938 [01:33<00:00, 10.00it/s]\n",
            "Loss=0.04024681821465492 Batch_id=0 Accuracy=96.88:   0%|          | 1/938 [00:00<01:37,  9.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 15\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0033372375182807446 Batch_id=937 Accuracy=99.30: 100%|██████████| 938/938 [01:34<00:00, 11.10it/s]\n",
            "Loss=0.0225546732544899 Batch_id=1 Accuracy=97.66:   0%|          | 2/938 [00:00<01:27, 10.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 9946/10000 (99.46%)\n",
            "\n",
            "EPOCH: 16\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.053173623979091644 Batch_id=937 Accuracy=99.33: 100%|██████████| 938/938 [01:36<00:00, 10.98it/s]\n",
            "Loss=0.01287767756730318 Batch_id=1 Accuracy=100.00:   0%|          | 1/938 [00:00<01:35,  9.84it/s]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 9945/10000 (99.45%)\n",
            "\n",
            "EPOCH: 17\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.009646359831094742 Batch_id=937 Accuracy=99.28: 100%|██████████| 938/938 [01:33<00:00,  9.99it/s]\n",
            "Loss=0.0028107864782214165 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:35,  9.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0138, Accuracy: 9954/10000 (99.54%)\n",
            "\n",
            "EPOCH: 18\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.07154850661754608 Batch_id=937 Accuracy=99.30: 100%|██████████| 938/938 [01:34<00:00, 10.99it/s]\n",
            "Loss=0.0015415129018947482 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:39,  9.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 19\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.11421487480401993 Batch_id=937 Accuracy=99.34: 100%|██████████| 938/938 [01:34<00:00,  9.95it/s]\n",
            "Loss=0.05371738225221634 Batch_id=0 Accuracy=98.44:   0%|          | 1/938 [00:00<01:45,  8.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0138, Accuracy: 9951/10000 (99.51%)\n",
            "\n",
            "EPOCH: 20\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.07680206000804901 Batch_id=937 Accuracy=99.31: 100%|██████████| 938/938 [01:34<00:00,  9.94it/s]\n",
            "Loss=0.02197420597076416 Batch_id=0 Accuracy=98.44:   0%|          | 1/938 [00:00<01:40,  9.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 9946/10000 (99.46%)\n",
            "\n",
            "EPOCH: 21\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0024631298147141933 Batch_id=937 Accuracy=99.34: 100%|██████████| 938/938 [01:34<00:00, 10.78it/s]\n",
            "Loss=0.0048365299589931965 Batch_id=1 Accuracy=99.22:   0%|          | 2/938 [00:00<01:31, 10.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 9947/10000 (99.47%)\n",
            "\n",
            "EPOCH: 22\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.005876655224710703 Batch_id=937 Accuracy=99.30: 100%|██████████| 938/938 [01:33<00:00, 10.08it/s]\n",
            "Loss=0.018464509397745132 Batch_id=1 Accuracy=100.00:   0%|          | 2/938 [00:00<01:26, 10.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 9945/10000 (99.45%)\n",
            "\n",
            "EPOCH: 23\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.001449784031137824 Batch_id=937 Accuracy=99.33: 100%|██████████| 938/938 [01:37<00:00, 10.68it/s]\n",
            "Loss=0.04383188486099243 Batch_id=0 Accuracy=98.44:   0%|          | 1/938 [00:00<01:33,  9.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 9952/10000 (99.52%)\n",
            "\n",
            "EPOCH: 24\n",
            "Learning rate : 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.015401684679090977 Batch_id=937 Accuracy=99.34: 100%|██████████| 938/938 [01:35<00:00,  9.82it/s]\n",
            "Loss=0.016537679359316826 Batch_id=1 Accuracy=99.22:   0%|          | 2/938 [00:00<01:29, 10.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 9950/10000 (99.50%)\n",
            "\n",
            "EPOCH: 25\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0015518602449446917 Batch_id=937 Accuracy=99.33: 100%|██████████| 938/938 [01:35<00:00,  9.79it/s]\n",
            "Loss=0.06440240889787674 Batch_id=0 Accuracy=95.31:   0%|          | 1/938 [00:00<01:37,  9.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 9945/10000 (99.45%)\n",
            "\n",
            "EPOCH: 26\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04015525430440903 Batch_id=937 Accuracy=99.36: 100%|██████████| 938/938 [01:34<00:00,  9.95it/s]\n",
            "Loss=0.004789422266185284 Batch_id=1 Accuracy=99.22:   0%|          | 1/938 [00:00<01:36,  9.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 9953/10000 (99.53%)\n",
            "\n",
            "EPOCH: 27\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.005802853964269161 Batch_id=937 Accuracy=99.40: 100%|██████████| 938/938 [01:35<00:00,  9.82it/s]\n",
            "Loss=0.006398696452379227 Batch_id=1 Accuracy=100.00:   0%|          | 1/938 [00:00<01:34,  9.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 9952/10000 (99.52%)\n",
            "\n",
            "EPOCH: 28\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.004761144984513521 Batch_id=937 Accuracy=99.37: 100%|██████████| 938/938 [01:33<00:00,  9.98it/s]\n",
            "Loss=0.011272403411567211 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:41,  9.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9945/10000 (99.45%)\n",
            "\n",
            "EPOCH: 29\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0029098214581608772 Batch_id=937 Accuracy=99.39: 100%|██████████| 938/938 [01:33<00:00, 10.07it/s]\n",
            "Loss=0.003757225116714835 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:38,  9.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 9950/10000 (99.50%)\n",
            "\n",
            "EPOCH: 30\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.005251748487353325 Batch_id=937 Accuracy=99.38: 100%|██████████| 938/938 [01:36<00:00, 10.37it/s]\n",
            "Loss=0.059150949120521545 Batch_id=0 Accuracy=98.44:   0%|          | 1/938 [00:00<01:33,  9.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 9947/10000 (99.47%)\n",
            "\n",
            "EPOCH: 31\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.000899007951375097 Batch_id=937 Accuracy=99.40: 100%|██████████| 938/938 [01:35<00:00,  9.83it/s]\n",
            "Loss=0.0074062589555978775 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:41,  9.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0133, Accuracy: 9949/10000 (99.49%)\n",
            "\n",
            "EPOCH: 32\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.007755790837109089 Batch_id=937 Accuracy=99.37: 100%|██████████| 938/938 [01:35<00:00,  9.86it/s]\n",
            "Loss=0.013353089801967144 Batch_id=1 Accuracy=100.00:   0%|          | 2/938 [00:00<01:28, 10.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9952/10000 (99.52%)\n",
            "\n",
            "EPOCH: 33\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.018600624054670334 Batch_id=937 Accuracy=99.41: 100%|██████████| 938/938 [01:35<00:00, 10.77it/s]\n",
            "Loss=0.001744627021253109 Batch_id=1 Accuracy=99.22:   0%|          | 1/938 [00:00<01:35,  9.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9952/10000 (99.52%)\n",
            "\n",
            "EPOCH: 34\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.016202658414840698 Batch_id=937 Accuracy=99.40: 100%|██████████| 938/938 [01:34<00:00,  9.88it/s]\n",
            "Loss=0.05885231867432594 Batch_id=0 Accuracy=98.44:   0%|          | 1/938 [00:00<02:21,  6.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0138, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 35\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.023323241621255875 Batch_id=937 Accuracy=99.34: 100%|██████████| 938/938 [01:34<00:00,  9.96it/s]\n",
            "Loss=0.010982813313603401 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:36,  9.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9949/10000 (99.49%)\n",
            "\n",
            "EPOCH: 36\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.16911892592906952 Batch_id=937 Accuracy=99.44: 100%|██████████| 938/938 [01:34<00:00,  9.94it/s]\n",
            "Loss=0.0015194768784567714 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:35,  9.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 9952/10000 (99.52%)\n",
            "\n",
            "EPOCH: 37\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.10705944150686264 Batch_id=937 Accuracy=99.39: 100%|██████████| 938/938 [01:34<00:00,  9.88it/s]\n",
            "Loss=0.004924635402858257 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:48,  8.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0139, Accuracy: 9949/10000 (99.49%)\n",
            "\n",
            "EPOCH: 38\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0012186591047793627 Batch_id=937 Accuracy=99.37: 100%|██████████| 938/938 [01:34<00:00,  9.94it/s]\n",
            "Loss=0.006604460999369621 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:37,  9.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 9951/10000 (99.51%)\n",
            "\n",
            "EPOCH: 39\n",
            "Learning rate : 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0803697481751442 Batch_id=937 Accuracy=99.38: 100%|██████████| 938/938 [01:35<00:00,  9.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 9949/10000 (99.49%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utzym3rKNT9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "8f8e5cdb-98a1-4a11-e297-6829a7960800"
      },
      "source": [
        "plot_misclassified(*mis_classfied(model, test_loader), 'Without L1 or L2 Norm')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAAJ5CAYAAABltN6GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvLEihQRERGRJ1akiIiIiDz9P5A0VbKvIQQtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpHO2YG2NWyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Only L1\n",
        "model = run_epochs(L1_weight_decay=0.0005, select_list=1)\n",
        "plot_misclassified(*mis_classfied(model, test_loader), 'Only L1 reg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM53aXc9NW0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only L2\n",
        "model = run_epochs(L2_weight_decay=0.0005, select_list=2)\n",
        "plot_misclassified(*mis_classfied(model, test_loader), 'Only L2 reg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHeIkVkZNllA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Both L2 and L1\n",
        "model = run_epochs(L1_weight_decay=0.0005, L2_weight_decay=0.0005, select_list=3)\n",
        "plot_misclassified(*mis_classfied(model, test_loader), 'Both L1 and L2 reg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy2gKYnWNsdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(test_losses, labels, title='Test Losses', ylabel='Loss', save=1)\n",
        "plot_history(train_losses, labels, title='Train Losses', ylabel='Loss')\n",
        "plot_history(test_acc, labels, title='Test Accuracy', ylabel='Accuracy')\n",
        "plot_history(train_acc, labels, title='Train Accuracy', ylabel='Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWVOTnOYGDzt",
        "colab_type": "code",
        "outputId": "4a070d79-30e2-405a-abdf-4a53c92fe080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# model =  Net().to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n",
        "# scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "\n",
        "# EPOCHS = 40\n",
        "# for epoch in range(EPOCHS):\n",
        "#     print(\"EPOCH:\", epoch)\n",
        "#     train(model, device, train_loader, optimizer, epoch)\n",
        "#     scheduler.step()\n",
        "#     test(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss=2.329705238342285 Batch_id=0 Accuracy=9.38:   0%|          | 1/938 [00:00<02:16,  6.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.10179602354764938 Batch_id=937 Accuracy=90.92: 100%|██████████| 938/938 [01:30<00:00, 10.33it/s]\n",
            "Loss=0.22025135159492493 Batch_id=1 Accuracy=96.88:   0%|          | 2/938 [00:00<01:30, 10.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1623, Accuracy: 9638/10000 (96.38%)\n",
            "\n",
            "EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.10102304071187973 Batch_id=937 Accuracy=97.16: 100%|██████████| 938/938 [01:32<00:00, 10.55it/s]\n",
            "Loss=0.06885883212089539 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:37,  9.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0964, Accuracy: 9853/10000 (98.53%)\n",
            "\n",
            "EPOCH: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.08699624985456467 Batch_id=937 Accuracy=97.28: 100%|██████████| 938/938 [01:34<00:00,  9.92it/s]\n",
            "Loss=0.10432171821594238 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:43,  9.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0908, Accuracy: 9826/10000 (98.26%)\n",
            "\n",
            "EPOCH: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.2448921799659729 Batch_id=937 Accuracy=97.34: 100%|██████████| 938/938 [01:34<00:00,  9.93it/s]\n",
            "Loss=0.12469275295734406 Batch_id=0 Accuracy=96.88:   0%|          | 1/938 [00:00<01:37,  9.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1022, Accuracy: 9819/10000 (98.19%)\n",
            "\n",
            "EPOCH: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.08562548458576202 Batch_id=937 Accuracy=97.43: 100%|██████████| 938/938 [01:33<00:00, 10.06it/s]\n",
            "Loss=0.07620199769735336 Batch_id=0 Accuracy=98.44:   0%|          | 1/938 [00:00<01:37,  9.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1033, Accuracy: 9783/10000 (97.83%)\n",
            "\n",
            "EPOCH: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.3328879773616791 Batch_id=937 Accuracy=97.44: 100%|██████████| 938/938 [01:33<00:00, 10.05it/s]\n",
            "Loss=0.13515867292881012 Batch_id=1 Accuracy=98.44:   0%|          | 1/938 [00:00<01:36,  9.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0936, Accuracy: 9757/10000 (97.57%)\n",
            "\n",
            "EPOCH: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0801144540309906 Batch_id=937 Accuracy=98.40: 100%|██████████| 938/938 [01:33<00:00, 10.08it/s]\n",
            "Loss=0.056562770158052444 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:36,  9.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0456, Accuracy: 9927/10000 (99.27%)\n",
            "\n",
            "EPOCH: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.14285321533679962 Batch_id=937 Accuracy=98.66: 100%|██████████| 938/938 [01:31<00:00, 10.22it/s]\n",
            "Loss=0.06287622451782227 Batch_id=1 Accuracy=99.22:   0%|          | 2/938 [00:00<01:31, 10.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0451, Accuracy: 9920/10000 (99.20%)\n",
            "\n",
            "EPOCH: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.056449998170137405 Batch_id=937 Accuracy=98.66: 100%|██████████| 938/938 [01:35<00:00,  9.81it/s]\n",
            "Loss=0.04297385364770889 Batch_id=1 Accuracy=99.22:   0%|          | 1/938 [00:00<01:34,  9.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0454, Accuracy: 9920/10000 (99.20%)\n",
            "\n",
            "EPOCH: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.13578082621097565 Batch_id=937 Accuracy=98.64: 100%|██████████| 938/938 [01:34<00:00,  9.94it/s]\n",
            "Loss=0.062267448753118515 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:35,  9.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0444, Accuracy: 9924/10000 (99.24%)\n",
            "\n",
            "EPOCH: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.06977657228708267 Batch_id=937 Accuracy=98.61: 100%|██████████| 938/938 [01:34<00:00,  9.96it/s]\n",
            "Loss=0.12205434590578079 Batch_id=0 Accuracy=96.88:   0%|          | 1/938 [00:00<01:34,  9.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0481, Accuracy: 9921/10000 (99.21%)\n",
            "\n",
            "EPOCH: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.17839789390563965 Batch_id=937 Accuracy=98.57: 100%|██████████| 938/938 [01:34<00:00, 10.77it/s]\n",
            "Loss=0.05496680736541748 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:39,  9.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0471, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "EPOCH: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.24645422399044037 Batch_id=937 Accuracy=98.82: 100%|██████████| 938/938 [01:34<00:00, 10.83it/s]\n",
            "Loss=0.0550847202539444 Batch_id=0 Accuracy=100.00:   0%|          | 1/938 [00:00<01:36,  9.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0402, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "EPOCH: 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.06069521605968475 Batch_id=937 Accuracy=98.89: 100%|██████████| 938/938 [01:34<00:00,  9.96it/s]\n",
            "Loss=0.21798303723335266 Batch_id=1 Accuracy=96.88:   0%|          | 2/938 [00:00<01:32, 10.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0404, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "EPOCH: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.35944366455078125 Batch_id=937 Accuracy=98.90: 100%|██████████| 938/938 [01:34<00:00, 11.09it/s]\n",
            "Loss=0.12193288654088974 Batch_id=0 Accuracy=96.88:   0%|          | 1/938 [00:00<01:38,  9.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0393, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "EPOCH: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.11115071922540665 Batch_id=937 Accuracy=98.83: 100%|██████████| 938/938 [01:34<00:00,  9.96it/s]\n",
            "Loss=0.13302990794181824 Batch_id=0 Accuracy=96.88:   0%|          | 1/938 [00:00<01:40,  9.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0408, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "EPOCH: 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0483454130589962 Batch_id=540 Accuracy=98.74:  58%|█████▊    | 541/938 [00:53<00:41,  9.52it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4A9WGSZGUQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %matplotlib inline\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "# axs[0, 0].plot(train_losses)\n",
        "# axs[0, 0].set_title(\"Training Loss\")\n",
        "# axs[1, 0].plot(train_acc[4000:])\n",
        "# axs[1, 0].set_title(\"Training Accuracy\")\n",
        "# axs[0, 1].plot(test_losses)\n",
        "# axs[0, 1].set_title(\"Test Loss\")\n",
        "# axs[1, 1].plot(test_acc)\n",
        "# axs[1, 1].set_title(\"Test Accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ7QFC6dGD3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.plot(train_epoch_end)\n",
        "# plt.plot(test_acc)\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('y label')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'validation'], loc='upper left')\n",
        "# plt.show()\n",
        "# print(max(train_epoch_end[:15]))\n",
        "# print(max(test_acc[:15]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKoWB7muKQn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvrSVM8fMIUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQNDUERqMI2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P7ALukuMI6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}